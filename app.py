import os
import re
import feedparser
import trafilatura
import streamlit as st
from openai import OpenAI

# --- Secrets / API Key ---
API_KEY = st.secrets.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
if not API_KEY:
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Cloud Secretsì— í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=API_KEY)

# --- Settings ---
FEED_URLS = [
    "https://feeds.reuters.com/reuters/businessNews",
    # í•„ìš”ì‹œ ì¶”ê°€: "https://feeds.reuters.com/reuters/worldNews",
]
DEFAULT_KEYWORDS = "tariff|tariffs|duty|duties|section 301|section 232|anti-dumping|countervailing|retaliatory tariff"

st.set_page_config(page_title="US Tariff News (Reuters)", layout="wide")
st.title("ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ê´€ì„¸ ê´€ë ¨ ë¡œì´í„° ë‰´ìŠ¤ ëª¨ë‹ˆí„°")

with st.sidebar:
    st.header("í•„í„°")
    kw_pattern = st.text_input("í‚¤ì›Œë“œ(ì •ê·œì‹ | ë¡œ êµ¬ë¶„)", DEFAULT_KEYWORDS)
    must_contain_us = st.checkbox("ë¯¸êµ­ ê´€ë ¨(US/United States/U.S.) í¬í•¨ë§Œ ë³´ê¸°", value=True)
    max_articles = st.slider("ìš”ì•½í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 1, 20, 6, 1)
    st.caption("â€» ìš”ì•½ì€ OpenAI API í˜¸ì¶œ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤.")

@st.cache_data(ttl=600)
def load_feed_entries(feed_urls):
    entries = []
    for url in feed_urls:
        feed = feedparser.parse(url)
        for e in feed.entries:
            entries.append({
                "title": e.get("title", ""),
                "link": e.get("link", ""),
                "summary": e.get("summary", ""),
            })
    return entries

def match_article(entry, pattern, require_us):
    title = (entry["title"] or "").lower()
    summary = (entry["summary"] or "").lower()
    text = title + " " + summary
    if not re.search(pattern, text, flags=re.IGNORECASE):
        return False
    if require_us and not re.search(r"\b(united states|u\.s\.|u\.s|us)\b", text, flags=re.IGNORECASE):
        # ë³¸ë¬¸ê¹Œì§€ í™•ì¸ ìœ„í•´ ê°„ë‹¨ fetch
        html = trafilatura.fetch_url(entry["link"])
        body = (trafilatura.extract(html) or "").lower() if html else ""
        if not re.search(r"\b(united states|u\.s\.|u\.s|us)\b", body, flags=re.IGNORECASE):
            return False
    return True

def fetch_body(url):
    html = trafilatura.fetch_url(url)
    return trafilatura.extract(html) if html else ""

def summarize_with_gpt(title, body):
    # ë³¸ë¬¸ì´ ì§§ê±°ë‚˜ ë¹„ì–´ ìˆì–´ë„ ì œëª© ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
    prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ 'ì „ë¬¸ ë³´ë„ê¸°ì‚¬ ìš”ì•½' ìŠ¤íƒ€ì¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
- 3ì¤„ ìš”ì•½(ì‚¬ì‹¤ ìœ„ì£¼, ê³¼ì¥ ê¸ˆì§€)
- í•µì‹¬ í¬ì¸íŠ¸ 3~5ê°œ
- ê´€ì„¸/ë¬´ì—­ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ë°˜ì˜(ì˜ˆ: 301/232, anti-dumping ë“±)

[ì œëª©]
{title}

[ë³¸ë¬¸]
{body[:3500]}
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.2,
        messages=[
            {"role":"system","content":"ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ ì—ë””í„°ì…ë‹ˆë‹¤. ì •í™•Â·ê°„ê²°Â·ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."},
            {"role":"user","content": prompt}
        ],
    )
    return resp.choices[0].message.content

entries = load_feed_entries(FEED_URLS)

# í•„í„°ë§
pattern = kw_pattern if kw_pattern.strip() else DEFAULT_KEYWORDS
filtered = []
for e in entries:
    try:
        if match_article(e, pattern, must_contain_us):
            filtered.append(e)
    except Exception:
        pass

st.caption(f"í•„í„°ë§ ê²°ê³¼: {len(filtered)}ê±´")

# ìƒìœ„ Nê±´ ìš”ì•½
for idx, e in enumerate(filtered[:max_articles], start=1):
    st.subheader(f"{idx}. {e['title']}")
    st.caption(e["link"])
    body = fetch_body(e["link"]) or ""
    summary = summarize_with_gpt(e["title"], body)
    st.write(summary)
    st.markdown("---")